# 协同过滤算法

In [2]:

```
import numpy as np 
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# import missingno as msno
from datetime import datetime
```

In [3]:

```
a = pd.read_table('./ratings.dat',sep = '::',header=None)
a
/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.
  """Entry point for launching an IPython kernel.
```

Out[3]:

|         |    0 |    1 |    2 |         3 |
| ------: | ---: | ---: | ---: | --------: |
|       0 |    1 | 1193 |    5 | 978300760 |
|       1 |    1 |  661 |    3 | 978302109 |
|       2 |    1 |  914 |    3 | 978301968 |
|       3 |    1 | 3408 |    4 | 978300275 |
|       4 |    1 | 2355 |    5 | 978824291 |
|     ... |  ... |  ... |  ... |       ... |
| 1000204 | 6040 | 1091 |    1 | 956716541 |
| 1000205 | 6040 | 1094 |    5 | 956704887 |
| 1000206 | 6040 |  562 |    5 | 956704746 |
| 1000207 | 6040 | 1096 |    4 | 956715648 |
| 1000208 | 6040 | 1097 |    4 | 956715569 |

1000209 rows × 4 columns

In [3]:

```
a.columns=['用户ID','电影ID','评分','时间戳']
a.sample(5)
```

In [7]:

```
a.info()  # 全部整数型
```

In [4]:

```
b=a.copy()
b.head()
```

## 数据清洗与探索

### 空值探索

In [9]:

```
b.isnull().sum()  # 无空值
```

Out[9]:

```
用户ID    0
电影ID    0
评分      0
时间戳     0
dtype: int64
```

### 时间格式转换

In [ ]:

```
# 对时间戳的转化，要注意一些方法只用于单个值得转化，像datetime.fromtimestamp()；
# 一些用于整列数据的转化，不能用于单个值，例如处理时间序列的dt系列的方法,(对于datetime，date,等类型的时间格式，他们的属性方法，dt,d,等基本都用于整列数据的处理)
# 对于单个值如要提取year，month，直接year，month函数即可.( date.fromtimestamp(b["时间戳"][0]).year )
```

In [5]:

```
b["时间戳"]=pd.to_datetime(b["时间戳"], unit='s')  #  unit='s'表示解析单位是秒
b
```

In [6]:

```
b["date"]=pd.to_datetime( b["时间戳"].dt.date )  
b["hour"]=b["时间戳"].dt.hour
b
```

In [112]:

```
b.info()
```

### 特征选择

协同过滤中，时间特征无用，因此可以去除

In [7]:

```
c=b[["用户ID","电影ID","评分"]]
c
```

### 重复值

In [115]:

```
c.duplicated().any()
```

Out[115]:

```
False
```

In [126]:

```
c["用户ID"].duplicated().any()  # 存在用户看了多个电影
```

Out[126]:

```
True
```

### 异常值观察

In [116]:

```
c.describe()
```

## 构建评分矩阵

In [8]:

```
d=c.pivot("用户ID","电影ID","评分")
d
```

### 空值填充

In [ ]:

```
# d.replace(np.nan,0)  将nan替换成0。
# fillna（）进行填充
```

In [9]:

```
e=d.fillna(0)
e
```

In [125]:

```
e.shape   # 6040个用户，4706部电影
```

Out[125]:

```
(6040, 3706)
```

### 余弦相似度评分

In [10]:

```
from sklearn.metrics.pairwise import cosine_similarity   # 调用余弦相似度计算方法
```

In [11]:

```
f=e.copy()#.values  # 电影评分矩阵，用以后续构建用户相似度矩阵
f
```

In [12]:

```
g=pd.DataFrame(cosine_similarity(f),index=f.index,columns=f.index)   # 输出用户相似度矩阵
g
```

### 构建基于用户协同过滤的函数

根据用户-项目评分矩阵计算用户之间的相似度。计算相似度常用的方法有余弦算法、修正余弦算法、皮尔森算法等等

In [128]:

```
# # 构建评分函数

# def ag(userid,itemid,k=10):
#     score=0
#     weight=0
#     fitem=f.loc[[userid],:]   # 指定的用户对所有电影的评分
#     itemf=f[[itemid]]         # 指定的电影的所有评分
    
#     guser=g[[userid]]         # 指定用户与其他用户的相似度
#     gusermaxindex= guser.nlargest(k,userid).iloc[1:,:].index    # 与指定用户最相似的若干用户的userid
#     for x in gusermaxindex:
#         if itemf.loc[x,itemid] != 0:
#             score+=guser.loc[x,userid]*itemf.loc[x,itemid]
#             weight+=guser.loc[x,userid]
#     if weight== 0:
#         return 0
#     else:
#         return round(score/float(weight),3) 

# #加权评分计算：
# #【（用户1与用户2相似度 * 用户2对指定电影的评分）+（用户1与用户3相似度 * 用户3对指定电影的评分）】/ (用户1与用户2相似度 + 用户1与用户3相似度)
```

上面这个使用DataFrame类型数据进行计算，但是速度较慢，一般来说对元组类型数据运算更快，因此采用下面的元组计算方法更快

In [149]:

```
def ag(userid,itemid,k=9):   # 注意这里的userid和itemid指的是真实的用户名和商品名，不是索引号
    score=0
    weight=0
    fitem=f.loc[userid].values   # 指定的用户对所有电影的评分
    itemf=f[itemid].values         # 指定的电影的所有评分
    
    guser=g.loc[userid].values         # 指定用户与其他用户的相似度
    gusermaxindex=np.argsort(guser)[-(k+1):-1] 
    #gusermaxindex= guser.nlargest(k,userid).iloc[1:,:].index    # 与指定用户最相似的若干用户的userid
    for x in gusermaxindex:
        if itemf[x] != 0:
            score+=guser[x]*itemf[x]
            weight+=guser[x]
    if weight== 0:
        return 0
    else:
        return score/float(weight) 

# 加权评分计算：
# 【（用户1与用户2相似度 * 用户2对指定电影的评分）+（用户1与用户3相似度 * 用户3对指定电影的评分）】/ (用户1与用户2相似度 + 用户1与用户3相似度)
```

In [150]:

```
ag(1,1)
```

Out[150]:

```
4.876821276375716
```

In [151]:

```
# 构建评分函数的循环评分--对用户和电影进行循环评分得到推荐评分矩阵
#gh=pd.DataFrame(np.zeros((f.shape[0],f.shape[1])),index=f.index,columns=f.columns)
def aaa(f):
    gh=pd.DataFrame(np.zeros((f.shape[0],f.shape[1])),index=f.index,columns=f.columns)
    for x in f.index[:10]:   # 用户太多，因此暂时选取10个用户用于验证计算是否跑通
        print(x)
        for y in f.columns:
            gh.loc[x,y]=ag(x,y)
    return gh
```

In [153]:

```
# 得到用户对多有电影的预测评分矩阵

hk=aaa(f)   # 全数据过大，需要计算出2400万个值，因此，只选取10个人进行运算（合计4万次运算）
hk
```

### 最终的评分推荐矩阵及规整

In [157]:

```
hkk=hk.stack().reset_index()
hkk.columns=["用户id","电影id","评分"]
hkk
```

In [166]:

```
# 经过规整，得到最终的推荐列表
hkk.groupby("用户id").apply(lambda x:x.nlargest(3,"评分")).drop(["用户id"],axis=1).droplevel(1,0).reset_index()
  
```

In [ ]:

```

```

# 聚类算法

In [2]:

```
import pandas as pd
import numpy as np
# import missingno as msno
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
```

In [3]:

```
movies = pd.read_table('/home/kesci/input/dainying7190/movies.dat',sep = '::',header=None)
ratings = pd.read_table('/home/kesci/input/dainying7190/ratings.dat',sep = '::',header=None)
users = pd.read_table('/home/kesci/input/dainying7190/users.dat',sep = '::',header=None)
ratings.columns=['用户ID','电影ID','评分','时间戳']
movies.columns=['电影ID','上市年份','种类']
users.columns=['用户ID','性别','年龄','职业','邮编']
/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.
  """Entry point for launching an IPython kernel.
/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.
  
/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.
  This is separate from the ipykernel package so we can avoid doing imports until
```

In [170]:

```
movies
```

In [219]:

```
ratings
```

In [172]:

```
users
```

## 数据清洗与探索

### 重复值

In [174]:

```
ratings.duplicated().any()  
```

Out[174]:

```
False
```

In [175]:

```
movies.duplicated().any()  
```

Out[175]:

```
False
```

In [176]:

```
users.duplicated().any()  
```

Out[176]:

```
False
```

### 空值

In [179]:

```
ratings.isnull().sum()
```

In [180]:

```
movies.isnull().sum()
```

In [181]:

```
users.isnull().sum()
```

### 异常值

In [185]:

```
movies.describe().T
```

In [186]:

```
users.describe().T
```

In [187]:

```
ratings.describe().T
```

### 格式调整

ratings表的时间格式需要调整为日期格式(rating)

In [4]:

```
ratings["时间戳"]=pd.to_datetime(ratings["时间戳"],unit="s")
ratings["date"]=pd.to_datetime(ratings["时间戳"].dt.date)
ratings.sample(10)
```

In [221]:

```
ratings.info()
```

In [5]:

```
rating=ratings[["用户ID","电影ID","评分","date"]]
rating
```

user表的性别需要调整为数字（user）

In [222]:

```
users.sample(10)
```

In [6]:

```
from sklearn import preprocessing   #p#预处理的包  涉及独热编码、标签编码，归一化，标准化等操作
lbl = preprocessing.LabelEncoder()    #将数字型或者非数字型标签转化为 0--(类个数-1)范围之内
users['性别']=lbl.fit_transform(users['性别'])
users.sample(10)
```

In [7]:

```
user=users[["用户ID","性别","年龄","职业"]]
user
```

movies表需要提取上市年份，分割所属种类

In [300]:

```
movies.head()
```

Out[300]:

|      | 电影ID |                           上市年份 |                           种类 |
| :--- | -----: | ---------------------------------: | -----------------------------: |
| 0    |      1 |                   Toy Story (1995) |  Animation\|Children's\|Comedy |
| 1    |      2 |                     Jumanji (1995) | Adventure\|Children's\|Fantasy |
| 2    |      3 |            Grumpier Old Men (1995) |                Comedy\|Romance |
| 3    |      4 |           Waiting to Exhale (1995) |                  Comedy\|Drama |
| 4    |      5 | Father of the Bride Part II (1995) |                         Comedy |

In [8]:

```
movies['上市年份'].str.split("(").map(lambda x: x[1].split(")")[0])   # astype函数适用于np数组
# 提取年份时，用split的方法有些麻烦，考虑换一种方式（使用pd中的str.extract()方法）
```

In [9]:

```
movies['上市年份']=movies['上市年份'].str.extract(r'(\d+)').astype(int)
movies.head()
```

In [10]:

```
dy=[]
for i in movies["种类"]:
    dy.extend(i.split("|"))  # list.append(object) 向列表中添加一个对象object;   list.extend(sequence) 把一个序列seq的内容添加到列表中
dyy=pd.unique(dy)            # unique()是以 数组形式（numpy.ndarray）返回所选列的所有唯一值（特征的所有唯一值）；nunique() 即返回的是唯一值的个数
dyy
```

Out[10]:

```
array(['Animation', "Children's", 'Comedy', 'Adventure', 'Fantasy',
       'Romance', 'Drama', 'Action', 'Crime', 'Thriller', 'Horror',
       'Sci-Fi', 'Documentary', 'War', 'Musical', 'Mystery', 'Film-Noir',
       'Western'], dtype=object)
```

In [11]:

```
moviess=pd.DataFrame(np.zeros((len(movies.index),len(dyy))),columns=dyy)
moviess
```

In [12]:

```
for x,y in enumerate(movies["种类"]):
    a=moviess.columns.get_indexer(y.split("|"))   # a.get_indexer(b),该函数用于返回b中字符在a中的索引号
    moviess.iloc[x,a]=1
moviess   
```

In [13]:

```
movie=movies.join(moviess)
movie=movie.drop(axis=1,columns="种类")
movie
```

### 表连接

In [14]:

```
join1=pd.merge(rating,user,left_on="用户ID",right_on="用户ID",how="left")
join1
```

In [15]:

```
join2=pd.merge(join1,movie,left_on="电影ID",right_on="电影ID",how="left")
join2
# 合成了最终的用户电影评分表
```

## 建模

### 数据标准化，归一化处理

In [16]:

```
from sklearn.preprocessing import MinMaxScaler  # 最大最小值的标准归一化方法
```

In [17]:

```
mms = MinMaxScaler()
```

In [18]:

```
join3=join2.sample(10000)  # 选取一部分数据作为训练集
join3
```

In [19]:

```
join4=join3.iloc[:,3:]
join4["date"]=join4["date"].dt.year #.astype(str)  # 归一化只能对str或者数字进行，因此时间改为年（字符格式）
join4
```

In [20]:

```
data_norm = mms.fit_transform(join4.values)
data_norm
```

### 通过训练集确定最有K值

In [21]:

```
from sklearn.cluster import KMeans
from sklearn import metrics
from sklearn.metrics import silhouette_score
```

In [22]:

```
qq=[]
ww=[]
ee=[]

for k in range(2,20):
    model=KMeans(n_clusters=k
                 # init="k-means++",max_iter=500 
                 )  # 构造模型
    model.fit(data_norm) # 训练模型
    pre=model.predict(data_norm) # 输出模型预测结果,model.fit(data_norm).labels_
    ch=metrics.calinski_harabasz_score(data_norm,pre)  # calinski_harabasz_score CH分数（值越大，效果越好）
    ss=metrics.silhouette_score(data_norm,pre)   # 轮廓系数，越接近1越好
    qq.append(model.inertia_ )     # 每个点到簇质心的距离平方和,越小效果越好
    ww.append(ch)
    ee.append(ss)
```

In [23]:

```
plt.figure(figsize=(7,4))
plt.plot(range(2,20),qq,marker="o")

plt.figure(figsize=(7,4))
plt.plot(range(2,20),ww,marker="o")

plt.figure(figsize=(7,4))
plt.plot(range(2,20),ee,marker="o")

# 三种方式综合评估效果后，选取16作为聚类类别
```

Out[23]:

```
[<matplotlib.lines.Line2D at 0x7f858108bd30>]
```

![img](https://cdn.kesci.com/rt_upload/DD57B09BF9D447BC8E1C46EDF91EC570/qq5tlzb2zj.png)

![img](https://cdn.kesci.com/rt_upload/DD57B09BF9D447BC8E1C46EDF91EC570/qq5tlz6wmf.png)

![img](https://cdn.kesci.com/rt_upload/DD57B09BF9D447BC8E1C46EDF91EC570/qq5tlzm2ns.png)

### 建模聚类

因为考虑到算力和时间问题，因此不对所有数据进行建模计算，取10%的数据，即100000条数据进行最终的建模计算

In [24]:

```
join5=join2.sample(10000)  # 选取一部分数据
join5
```

In [25]:

```
join6=join5.iloc[:,1:]
join6["date"]=join6["date"].dt.year #.astype(str)  # 归一化只能对str或者数字进行，因此时间改为年（字符格式）
join6
```

In [27]:

```
model=KMeans(n_clusters=15
                 # init="k-means++",max_iter=500 
                 )  
model.fit(join6) 
print(model.n_clusters)
model.labels_
15
```

Out[27]:

```
array([12,  4,  6, ...,  1,  4,  1], dtype=int32)
```

In [28]:

```
join5["类别"]=model.labels_
join5
```

In [29]:

```
usertype=join5[["用户ID","类别"]]
usertype
```

In [30]:

```
kk=usertype.drop_duplicates()#["用户ID"].duplicated()
kk
```

In [31]:

```
kk.groupby("用户ID").agg({"类别":pd.Series.nunique})
```

In [34]:

```
kk[kk["用户ID"]==6]
```

### 构建推荐列表

基本思路是：将你没有看过的电影，你同一类群中大家又都喜欢的，推荐给你

- 1、用户没看过的电影ID列表【用户-电影（not）】
- 2、【用户-电影（not）】匹配用户类群【用户-电影（not）-类别】
- 3、用户对电影的喜好度【用户-电影-评分】
- 4、【用户-电影-评分】匹配用户类别【用户-电影-评分-类别】
- 5、同一类别中的用户对同一种电影的喜好度进行聚合，得到，用户类别对每一个电影的喜好度【类别-电影-平均评分】
- 6、将2与5进行合并（电影-类别），得到【用户-电影（not）-类别-类别平均评分】
- 7、对用户进行分组排序，个性化推荐

- 目前的几个缺点
- 1.由于基于的是用户看过的电影给同类中没看过该电影的人看，所以如果有电影在某一类群中没有任何人看过，则计算评分就是0，而这些电影实际上也是具有推荐价值的，在当前的算法中不会体现，这是该算法的一大缺点
- 2.在后续依据平均分进行计算时，存在有些小众电影，看的人少，但是评分很高，使得平均分很高，因此在实际中应该考虑加入对观影人数维度的考量。
- 3.对于用户的冷启动问题未涉及

In [33]:

```
# 1.构建用户对所有电影的对应表格（堆叠状态）
q=join5[["用户ID","电影ID","评分","类别"]]
q["是否看过"]=1
q
```

In [35]:

```
w=q.pivot("用户ID","电影ID","是否看过").fillna(0)
w
```

In [36]:

```
e=w.stack().reset_index()
e.rename(columns={0:'是否看过'},inplace=True)
e

# 以上两步，来回堆叠pivot展开是为了，构建用户对所有电影的对应表格（之前的表中，不包含用户没看过电影的纵向对应）
```

In [37]:

```
# 2 这些事用户没有看过的电影
r=e[e.是否看过==0].drop('是否看过',axis=1)  
r
```

In [38]:

```
t=r.merge(kk,how="left")  # 将用户没看过电影附加用户类别
t
```

In [39]:

```
#3、定义喜欢（用户对电影的评分表示喜好度），构建用户对电影的喜好度【用户-电影-评分】，因此从初始评分表抽取必要标签即可
y = q[["电影ID","评分","类别"]]
y
```

In [40]:

```
#5、同一类别中的用户对同一电影的评分进行聚合，得到，用户类别对每一个电影的评分【类别-电影-平均评分】  
u=y.groupby(['类别','电影ID']).mean()  #同类用户对同一个电影的的评分均值
u.reset_index(inplace=True)
u.head()
```

In [41]:

```
#6、将2与5进行合并,即用户没看过的电影在所属类群中，其他用户给到该电影评分的均值，作为是否向这个用户推荐这个电影的评分依据
i=pd.merge(t,u,on=['电影ID','类别'],how='left').fillna(0)
i.head()
```

- 这一步就是上面说的有些电影在某些类别中没人看过，因此求mean为0.
- 这个可以采用inner连接，但是考虑到没看过的电影评分为0，不影响其他评分，所以left连接也是可以得

In [426]:

```
i.sample(10)
```

In [42]:

```
i.groupby("用户ID").apply(lambda x: x.nlargest(3,"评分")) 
# 取到每个用户前几名评分的电影。

# 后续只需要对数据进行一定规整即可
```

In [ ]:

```

```

In [ ]:

```

```

### （修正后的方法）加上评分人数的考量

In [43]:

```
yy = q[["用户ID","电影ID","评分","类别"]]
yy
```

In [44]:

```
gty=yy.groupby(['类别','电影ID']).agg(均值=("评分","mean"),人数=("用户ID",pd.Series.nunique),人数1=("用户ID","count"))
gty.reset_index(inplace=True)
gty.head()
```

In [45]:

```
ii=pd.merge(t,gty,on=['电影ID','类别'],how='left').fillna(0)
ii.head()
```

In [46]:

```
jll=ii[(ii["均值"]>3)&(ii["人数"]>15) ]
jll
```

In [47]:

```
jll.groupby("用户ID").apply(lambda x: x.nlargest(3,"人数"))   # 得到评分>3,评分人数有一定要求的推荐列表
```

Out[47]:

|          |          | 用户ID | 电影ID |     类别 |     均值 | 人数 | 人数1 |
| -------: | -------: | -----: | -----: | -------: | -------: | ---: | ----: |
|   用户ID |          |        |        |          |          |      |       |
|        5 |      896 |      5 |   1580 |       14 | 3.640000 | 25.0 |  25.0 |
|      922 |        5 |   1617 |     14 | 4.000000 |     22.0 | 22.0 |       |
|     1002 |        5 |   1784 |     14 | 3.857143 |     21.0 | 21.0 |       |
|        6 |     7236 |      6 |   2858 |        5 | 4.303030 | 33.0 |  33.0 |
|     6831 |        6 |   2628 |      5 | 3.466667 |     30.0 | 30.0 |       |
|      ... |      ... |    ... |    ... |      ... |      ... |  ... |   ... |
|     6039 | 19325300 |   6039 |   1198 |        6 | 4.296296 | 27.0 |  27.0 |
| 19325430 |     6039 |   1270 |      6 | 4.250000 |     24.0 | 24.0 |       |
|     6040 | 19330484 |   6040 |   1196 |        6 | 4.294118 | 34.0 |  34.0 |
| 19330487 |     6040 |   1197 |      6 | 4.535714 |     28.0 | 28.0 |       |
| 19330490 |     6040 |   1198 |      6 | 4.296296 |     27.0 | 27.0 |       |